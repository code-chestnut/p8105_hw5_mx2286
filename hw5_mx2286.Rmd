---
title: "hw5_mx2286"
author: "William Xie"
date: "2024-11-15"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rvest)
library(broom)
library(purrr)
library(dplyr)
library(tidyr)
```

# Problem 1: The Birthday Paradox
```{r}

# Introduction
# Given n people, we want to determine the probability that at least two share a birthday. 
# Assume 365 uniformly distributed days and no leap years.

# Simulation Function
birthday_simulation <- function(n) {
  any(duplicated(sample(1:365, n, replace = TRUE)))
}

# Running Simulations
group_sizes <- 2:50
num_simulations <- 10000

probabilities <- sapply(group_sizes, function(n) {
  mean(replicate(num_simulations, birthday_simulation(n)))
})

# Results
plot(group_sizes, probabilities, type = 'b', pch = 19, col = 'blue',
     xlab = 'Group Size',
     ylab = 'Probability of Shared Birthday',
     main = 'Probability of Shared Birthdays vs. Group Size')

```
# The probability of at least two people sharing a birthday rises sharply with group size. 
# By group size 23, the probability exceeds 50%, a well-known result of the Birthday Paradox.
# As group size increases further, the probability approaches 1, making shared birthdays almost certain.


# Problem2
```{r 2-1: First set the following design elements}
set.seed(1)

simulate_p_value <- function(true_mean) {
  data <- rnorm(30, mean = true_mean, sd = 5)
  test <- t.test(data, mu = 0)
  broom::tidy(test) %>%
    dplyr::select(mu_hat = estimate, p_value = p.value)
}

test_results <- expand_grid(
  true_mean = 0:6,
  iter = 1:5000
) %>%
  rowwise() %>%
  mutate(simulation = list(simulate_p_value(true_mean))) %>%
  unnest(simulation) %>%
  mutate(indicator = as.integer(p_value < 0.05))
```

```{r 2-2: Make a plot showing the proportion of times}
# Calculate power by grouping results
power_results <- test_results %>%
  group_by(true_mean) %>%
  summarize(power = mean(p_value < 0.05), .groups = 'drop')

# Plot power curve
ggplot(power_results, aes(x = true_mean, y = power)) +
  geom_line() +
  geom_point() +
  labs(
    title = "Power Curve: Proportion of Null Rejections",
    x = "True Mean (Effect Size)",
    y = "Power"
  ) +
  theme_minimal()
```
The graph shows that as the true mean increases, the power rises and approaches 1. This indicates that larger effect sizes lead to greater statistical power.


```{r Make a plot showing the average estimate of 𝜇̂ }
library(ggplot2)

# Calculate average mu_hat
average_mu_hat <- test_results %>%
  group_by(true_mean) %>%
  summarize(
    avg_mu_hat_all = mean(mu_hat),
    avg_mu_hat_rejected = mean(mu_hat[p_value < 0.05]),
    .groups = 'drop'
  )

# Plot average mu_hat
ggplot(average_mu_hat, aes(x = true_mean)) +
  geom_line(aes(y = avg_mu_hat_all, color = "All samples")) +
  geom_point(aes(y = avg_mu_hat_all, color = "All samples")) +
  geom_line(aes(y = avg_mu_hat_rejected, color = "Rejected null")) +
  geom_point(aes(y = avg_mu_hat_rejected, color = "Rejected null")) +
  labs(
    title = "Average Estimate of μ̂ vs True μ",
    x = "True Mean",
    y = "Average Estimate of μ̂",
    color = "Sample Group"
  ) +
  theme_minimal()

```
The graph displays the average estimate of μ̂ across all samples and for samples where the null hypothesis was rejected. For smaller effect sizes, the sample average of μ̂ for tests rejecting the null is larger than that for all samples. This occurs because μ̂ values are skewed upward when the null is rejected, with some tests being rejected purely by chance. As the true mean (μ) increases, power improves, bias decreases, and the difference between the two averages diminishes.

## Problem 3
```{r Create a city_state variable}
library(readr)
library(dplyr)
library(stringr)
library(janitor)
# Load and preprocess homicide data
url <- "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"
homicide_data <- read_csv(url) %>% 
  clean_names() %>% 
  mutate(city_state = str_c(city, ", ", state)) %>% 
  filter(city_state != "Tulsa, AL")

summary_homicide <- homicide_data %>% 
  group_by(city_state) %>% 
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest"), na.rm = TRUE)
  ) %>% 
  filter(city_state != "Tulsa, AL")

summary_homicide |> knitr::kable()
```
The dataset contains r nrows(homicide_data) recorded homicides over the past decade across 50 of the largest American cities. It includes details such as the report date, victim demographics (name, race, age, and sex), location (city, state, latitude, and longitude), and whether an arrest was made. "Tulsa, AL" is excluded due to a data entry error.

```{r the proportion of homicides}
# Filter data for Baltimore
baltimore_data <- summary_homicide %>%
  filter(city_state == "Baltimore, MD")

# Perform proportion test for Baltimore
prop_test_result <- prop.test(
  x = pull(baltimore_data, unsolved_homicides),
  n = pull(baltimore_data, total_homicides)
) %>%
  broom::tidy()

# Function for proportion test
prop_test_function <- function(unsolved, total) {
  prop.test(x = unsolved, n = total) %>%
    broom::tidy() %>%
    select(estimate, conf.low, conf.high)
}

# Apply the proportion test function across all cities
prop_test_results <- summary_homicide %>%
  mutate(
    test_summary = map2(unsolved_homicides, total_homicides, prop_test_function)
  ) %>%
  unnest(test_summary) %>%
  rename(proportion = estimate)

# Display results
prop_test_results %>% knitr::kable()
prop_test_result %>% knitr::kable()

```
```{r Create a plot that shows the estimates and CIs for each city }
ggplot(prop_test_results, aes(x = reorder(city_state, -proportion), y = proportion)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  labs(
    title = "Proportion of unsolved homicides by city",
    x = "City",
    y = "Proportion of unsolved homicides"
  ) +
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 90, hjust = 0.5),
  axis.text.y = element_text(size = 8),
    plot.title = element_text(face = "bold", hjust = 0.5),
    plot.caption = element_text(hjust = 0))
```
Variation Across Cities: 
There is substantial variation in the proportion of unsolved homicides across different cities. 
Confidence intervals provide a range of uncertainty around the estimated proportions. 
Cities with wide intervals may have smaller sample sizes, resulting in less precise estimates.

Implications: 
High proportions of unsolved homicides could indicate challenges related to resources, 
socio-economic factors, or other underlying issues. Policymakers and law enforcement agencies can use this information to allocate resources more effectively.
